### Still to be divided into subtopics
* Distributed Representations of Words and Phrases and their Compositionality [link](https://arxiv.org/pdf/1310.4546.pdf). It discusses the subsampling techniques of removing common words like of, the which just act like noise in the data.
* 

1. ### Stemming
	In this step you reduce similar words to a single word. For example beautiful and beautifully are stemmed to beauti.
	Papers. Porter stemmer is the best stemming algorithm and also it can be modified according to your use cases also.
	* A Comparative Study of Stemming Algorithms [link](http://kenbenoit.net/assets/courses/tcd2014qta/readings/Jivani_ijcta2011020632.pdf). Find which algorithm interests you.
	* An algorithm for suffix stripping M.F. Porter 1980 [link](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.848.7219&rep=rep1&type=pdf)
	* The English (Porter2) stemming algorithm [link](http://snowball.tartarus.org/algorithms/english/stemmer.html)
	
2. ### Lematisation
	It is the process of reducing a group of words into their lemma or dictionary form. It takes into account their parts 	      of speech. Example good, better, best are lemmatised to good, good, good
	* Survey paper of Different Lemmatization Approaches [link](http://www.ijrat.org/downloads/icatest2015/ICATEST-2015127.pdf). This paper discusses various lemmatisation techniques, read what interests you.
	* Lemmatization for variation-rich languages using deep learning [link](https://academic.oup.com/dsh/article/32/4/797/2669790)
	All the papaers for this topic are referenced in the Survey paper, so choose the lemmatizer that interests you.
	
3. ### Word Embeddings
 	This is the technique used to represent languages in vector form of real numbers.
	* word2vec Parameter Learning Explained [link](https://arxiv.org/pdf/1411.2738.pdf)
	* Siamese CBOW: Optimizing Word Embeddings for Sentence Representations [link](http://aclweb.org/anthology/P/P16/P16-1089.pdf)
	* Distributed Representatiosn of Sentences and Documents [link](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)
	* Skip-Through Vectors [link](https://arxiv.org/pdf/1506.06726.pdf)
	* Towards universal Paraphrastic Sentence Embeddings [link](https://arxiv.org/pdf/1511.08198.pdf)
	
4. ### Part of Speech Tagging
 	It is making up of words in the sentence as noun, verb, adjective, adverb.
	* Dynamic Feature Induction: The Last Gist to the State-of-the-Art [link](https://aclweb.org/anthology/N16-1031.pdf)
	* Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models [link](https://transacl.org/ojs/index.php/tacl/article/viewFile/837/192)
	
5. ### Named Entity Disambiguation
 	It is the process of identifying the mentions of entities in a sentence. Example 'Apple earned a revenue of 400 USD 	    in 2017'. In this exmaple Apple is name of a compant and not a fruit.
	* Leveraging Deep Neural Networks and Knowledge Graphs for Entiity Disambiguation [link](https://arxiv.org/pdf/1504.07678.pdf)
	* Deep Joint Entity Disambiguation with Local Neural Attention [link](https://arxiv.org/pdf/1704.04920.pdf)
	
6. ### Named Entity Recognition
 	It is the task of identifying entities in a sentence and classifying then into categories like a person, 		organization, date, location.
	* Neural Architectures for Named Entity Recognition [link](https://arxiv.org/pdf/1603.01360.pdf)
	
